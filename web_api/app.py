# web_api/app.py

import os
import sys
import httpx 
import json
import time
from threading import Thread
from flask import Flask, render_template, jsonify, request
from flask_cors import CORS  # æ·»åŠ CORSæ”¯æŒï¼Œç”¨äºå¾®ä¿¡å°ç¨‹åºè·¨åŸŸè®¿é—®
from pymongo import MongoClient
from openai import OpenAI # <--- 1. ä½¿ç”¨ OpenAI åº“
from bs4 import BeautifulSoup

# ----------------------------------------------------
# !! å…³é”® !! ä¿®æ­£ Python å¯¼å…¥è·¯å¾„
# ----------------------------------------------------
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, '..'))
sys.path.append(project_root)

try:
    from scrapy.crawler import CrawlerProcess
    
    # !! å…³é”®ä¿®å¤ !!
    # æˆ‘ä»¬ä¸å†å¯¼å…¥ 'settings' äº†ï¼Œåªå¯¼å…¥çˆ¬è™«æœ¬èº«
    from university_scraper.university_scraper.spiders.pku_generic_spider import PkuGenericSpider
    
except ImportError as e:
    print(f"ä¸¥é‡é”™è¯¯: æ— æ³•å¯¼å…¥ Scrapy æ¨¡å—ã€‚è¯·ç¡®ä¿ 'university_scraper' æ–‡ä»¶å¤¹å­˜åœ¨ã€‚\n{e}")
    sys.exit(1)
# ----------------------------------------------------


# --- Flask åº”ç”¨åˆå§‹åŒ– ---
app = Flask(__name__)
# æ·»åŠ CORSæ”¯æŒï¼Œå…è®¸å¾®ä¿¡å°ç¨‹åºè·¨åŸŸè®¿é—®
CORS(app, resources={r"/api/*": {"origins": "*"}})  # å…è®¸æ‰€æœ‰æ¥æºè®¿é—®APIï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®é™åˆ¶ç‰¹å®šåŸŸåï¼‰
JOB_STATUS = {"status": "idle", "message": "ç³»ç»Ÿå°±ç»ªã€‚"}

# --- æ•°æ®åº“è¿æ¥ ---
try:
    client = MongoClient('mongodb://localhost:27017/')
    db = client.teacher_db
    recipe_collection = db.spider_recipes    
    teacher_collection = db.pku_cs_teachers  
    client.server_info()
    print("MongoDB è¿æ¥æˆåŠŸï¼")
except Exception as e:
    print(f"è­¦å‘Šï¼šæ— æ³•è¿æ¥åˆ° MongoDBã€‚\né”™è¯¯: {e}")

# --- DeepSeek AI å®¢æˆ·ç«¯åˆå§‹åŒ– (ä½¿ç”¨ OpenAI åº“) ---
try:
    # !! æ›¿æ¢æˆä½ çš„ DeepSeek Key !!
    DEEPSEEK_API_KEY = "sk-396fae50c55148f7a35875d11ba00f61" 
    if "YOUR_DEEPSEEK_API_KEY_HERE" in DEEPSEEK_API_KEY:
        print("è­¦å‘Š: è¯·åœ¨ app.py ä¸­å¡«å…¥ä½ çš„ DEEPSEEK_API_KEY")
    
    ai_client = OpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url="https://api.deepseek.com" # <--- æŒ‡å‘ DeepSeek
    )
except Exception as e:
    print(f"åˆå§‹åŒ– DeepSeek AI å®¢æˆ·ç«¯å¤±è´¥: {e}")
    ai_client = None

# ----------------------------------------------------
# é¡µé¢è·¯ç”±
# ----------------------------------------------------
@app.route('/')
def index():
    return render_template('index.html')

# ----------------------------------------------------
# API è·¯ç”±
# ----------------------------------------------------
@app.route('/api/start-full-automation', methods=['POST'])
def start_full_automation():
    global JOB_STATUS
    if JOB_STATUS.get("status") == "running":
        return jsonify({'status': 'error', 'message': 'ä¸€ä¸ªä»»åŠ¡å·²ç»åœ¨è¿è¡Œä¸­ï¼è¯·ç­‰å¾…å®ƒå®Œæˆã€‚'}), 400
    data = request.json
    master_url = data.get('master_url')
    if not master_url:
        return jsonify({'status': 'error', 'message': 'æœªæä¾›å¤§å­¦é™¢ç³»ä¸»é¡µ URL'}), 400
    JOB_STATUS = {"status": "running", "message": "ä»»åŠ¡å·²å¯åŠ¨..."}
    app_context = app.app_context()
    thread = Thread(target=run_full_automation, args=(app_context, master_url))
    thread.daemon = True 
    thread.start()
    return jsonify({'status': 'success', 'message': 'å…¨æµç¨‹è‡ªåŠ¨åŒ–å·²åœ¨åå°å¯åŠ¨ï¼'})

@app.route('/api/check-status', methods=['GET'])
def check_status():
    global JOB_STATUS
    return jsonify(JOB_STATUS)

@app.route('/api/search', methods=['GET'])
def search_teachers():
    # (æ­¤å‡½æ•°å’Œä¹‹å‰ä¸€æ ·ï¼Œæ— éœ€æ”¹åŠ¨)
    try:
        query_name = request.args.get('name')
        query_college = request.args.get('college')
        query_research = request.args.get('research')
        mongo_filter = {}
        if query_name:
            mongo_filter['name'] = {'$regex': query_name, '$options': 'i'}
        if query_college:
            mongo_filter['department'] = {'$regex': query_college, '$options': 'i'}
        if query_research:
            mongo_filter['research_interests'] = {'$regex': query_research, '$options': 'i'}
        teachers_cursor = teacher_collection.find(mongo_filter, {'_id': 0}).limit(100)
        teachers_list = list(teachers_cursor)
        return jsonify({'status': 'success', 'data': teachers_list})
    except Exception as e:
        return jsonify({'status': 'error', 'message': str(e)}), 500

# ----------------------------------------------------
# å…¨æµç¨‹è‡ªåŠ¨åŒ–çš„â€œçœŸæ­£â€æ‰§è¡Œå‡½æ•°
# ----------------------------------------------------
# ----------------------------------------------------
# å…¨æµç¨‹è‡ªåŠ¨åŒ–çš„â€œçœŸæ­£â€æ‰§è¡Œå‡½æ•° (!! å®Œæ•´ç‰ˆ !!)
# ----------------------------------------------------
def run_full_automation(app_context, master_url):
    global JOB_STATUS
    with app_context:
        try:
            print("\n" + "="*50)
            print("ğŸ¤– [å…¨æµç¨‹è‡ªåŠ¨åŒ–ä»»åŠ¡å·²å¯åŠ¨]")
            print("="*50 + "\n")
            
            # ----------------------------------------------------
            # [æ­¥éª¤ 1: AI 1 çˆ¬å–æ‰€æœ‰é™¢ç³»åˆ—è¡¨]
            # ----------------------------------------------------
            JOB_STATUS = {"status": "running", "message": f"æ­¥éª¤ 1: æ­£åœ¨ä» {master_url} å¯»æ‰¾é™¢ç³»åˆ—è¡¨..."}
            print(JOB_STATUS["message"])
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ...'}
            with httpx.Client(headers=headers, follow_redirects=True, timeout=10.0) as http_client:
                response = http_client.get(master_url)
                soup = BeautifulSoup(response.text, 'html.parser')
            links_text = ""
            for a in soup.find_all('a', href=True):
                text = a.get_text(strip=True)
                if text and len(text) < 50:
                    links_text += f"<a href=\"{a['href']}\">{text}</a>\n"
            
            prompt_ai_1 = f"""
            ä½ æ˜¯ä¸€ä¸ªç½‘ç«™å¯¼èˆªåˆ†ææœºå™¨äººã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¤§å­¦å®˜ç½‘çš„â€œé™¢ç³»è®¾ç½®â€é¡µé¢çš„æ‰€æœ‰é“¾æ¥ã€‚
            è¯·åˆ†æè¿™äº›é“¾æ¥ï¼Œæ‰¾å‡ºæ‰€æœ‰ã€å­¦æœ¯é™¢ç³»ã€‘ï¼ˆä¾‹å¦‚ï¼šæ•°å­¦ç§‘å­¦å­¦é™¢ã€ç‰©ç†å­¦é™¢ã€ä¿¡æ¯ç§‘å­¦æŠ€æœ¯å­¦é™¢ï¼‰ï¼Œå¹¶å¿½ç•¥â€œå›¾ä¹¦é¦†â€ã€â€œåå‹¤éƒ¨â€ã€â€œè§„ç« åˆ¶åº¦â€ç­‰éå­¦æœ¯éƒ¨é—¨ã€‚

            é“¾æ¥åˆ—è¡¨ (åªæˆªå–å‰15000å­—ç¬¦):
            {links_text[:15000]}

            è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰å­¦æœ¯é™¢ç³»çš„åˆ—è¡¨ï¼š
            {{
              "departments": [
                {{ "name": "æ•°å­¦ç§‘å­¦å­¦é™¢", "url": "..." }},
                {{ "name": "ç‰©ç†å­¦é™¢", "url": "..." }}
              ]
            }}
            """
            
            print("  æ­¥éª¤ 1.1: æ­£åœ¨è¯·æ±‚ AI 1 åˆ†æé™¢ç³»åˆ—è¡¨...")
            completion = ai_client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt_ai_1}],
                response_format={"type": "json_object"}
            )
            response_data = json.loads(completion.choices[0].message.content)
            college_list_raw = response_data.get('departments', [])
            if not college_list_raw:
                raise Exception("AI 1 æœªèƒ½ä»ä¸»é¡µä¸Šæ‰¾åˆ°é™¢ç³»åˆ—è¡¨")

            college_list = []
            for college in college_list_raw:
                name = college.get('name')
                url_relative = college.get('url')
                if name and url_relative:
                    absolute_url = httpx.URL(master_url).join(url_relative)
                    college_list.append({'name': name, 'homepage_url': str(absolute_url)})
            
            JOB_STATUS = {"status": "running", "message": f"æ­¥éª¤ 1 æˆåŠŸï¼šAI 1 æ‰¾åˆ° {len(college_list)} ä¸ªé™¢ç³»ã€‚"}
            print(JOB_STATUS["message"])

            # ----------------------------------------------------
            # [æ­¥éª¤ 2 & 3: AI 2/3 å¾ªç¯ç”Ÿæˆé…æ–¹]
            # ----------------------------------------------------
            total = len(college_list)
            for i, college in enumerate(college_list):
                JOB_STATUS = {"status": "running", "message": f"æ­¥éª¤ 2/3 ({i+1}/{total}): æ­£åœ¨ä¸º {college['name']} ç”Ÿæˆé…æ–¹..."}
                print(f"\n{JOB_STATUS['message']}")
                try:
                    # --- æ­¥éª¤ 2: AI 2 æ‰¾å¸ˆèµ„é¡µ ---
                    print(f"  æ­¥éª¤ 2: æ­£åœ¨ä¸‹è½½ {college['name']} çš„ä¸»é¡µå¹¶å¯»æ‰¾â€œå¸ˆèµ„â€é“¾æ¥...")
                    with httpx.Client(headers=headers, follow_redirects=True, timeout=10.0) as http_client:
                        response = http_client.get(college['homepage_url'])
                    soup = BeautifulSoup(response.text, 'html.parser')
                    links_text = ""
                    for a in soup.find_all('a', href=True):
                        text = a.get_text(strip=True)
                        if text and len(text) < 30:
                            links_text += f"<a href=\"{a['href']}\">{text}</a>\n"

                    prompt_ai_2 = f"""
                    ä½ æ˜¯ä¸€ä¸ªç½‘å€å¯¼èˆªæœºå™¨äººã€‚
                    ä¸‹é¢æ˜¯â€œ{college['name']}â€ä¸»é¡µä¸Šçš„æ‰€æœ‰é“¾æ¥ã€‚
                    è¯·ä»ä¸­æ‰¾å‡ºä¸€ä¸ªã€æœ€å¯èƒ½ã€‘æŒ‡å‘â€œå¸ˆèµ„é˜Ÿä¼â€ã€â€œæ•™èŒå·¥â€ã€â€œå­¦è€…æ•™æˆâ€åˆ—è¡¨çš„é“¾æ¥ï¼ˆhrefï¼‰ã€‚
                    
                    é“¾æ¥åˆ—è¡¨:
                    {links_text[:10000]}

                    è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¿”å›ï¼Œåªè¿”å›é‚£ä¸ªæœ€ç›¸å…³çš„URLï¼š
                    {{
                      "faculty_url": "..."
                    }}
                    """
                    completion = ai_client.chat.completions.create(
                        model="deepseek-chat",
                        messages=[{"role": "user", "content": prompt_ai_2}],
                        response_format={"type": "json_object"}
                    )
                    faculty_url_relative = json.loads(completion.choices[0].message.content).get('faculty_url')
                    if not faculty_url_relative:
                        print(f"  > AI 2 æœªèƒ½æ‰¾åˆ°â€œå¸ˆèµ„â€é“¾æ¥ã€‚è·³è¿‡ {college['name']}ã€‚")
                        continue
                    
                    faculty_url = str(httpx.URL(college['homepage_url']).join(faculty_url_relative))
                    print(f"  > AI 2 æ‰¾åˆ°å¸ˆèµ„é¡µ: {faculty_url}")
                    
                    # --- æˆæœ¬èŠ‚çº¦æ£€æŸ¥ ---
                    existing_recipe = recipe_collection.find_one({"start_url": faculty_url})
                    if existing_recipe:
                        print(f"  > æ•°æ®åº“ä¸­å·²å­˜åœ¨æ­¤é…æ–¹ã€‚è·³è¿‡ AI 3ã€‚")
                        continue

                    # --- æ­¥éª¤ 3: AI 3 ç”Ÿæˆé…æ–¹ ---
                    recipe = _generate_recipe_logic(faculty_url, college['name']) # è°ƒç”¨è¾…åŠ©å‡½æ•°
                    
                    # --- æ­¥éª¤ 3.5: ä¿å­˜é…æ–¹ ---
                    recipe_collection.update_one(
                        {'start_url': recipe['start_url']},
                        {'$set': recipe},
                        upsert=True
                    )
                    print(f"  > AI 3 æˆåŠŸ: {college['name']} çš„é…æ–¹å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼")
                    time.sleep(3) # ä¼‘æ¯ 3 ç§’
                except Exception as e_inner:
                    print(f"  > å¤„ç† {college['name']} æ—¶å‡ºé”™: {e_inner}")

            # ----------------------------------------------------
            # [æ­¥éª¤ 4: è‡ªåŠ¨å¯åŠ¨ Scrapy (ç¡¬ç¼–ç )]
            # ----------------------------------------------------
            JOB_STATUS = {"status": "running", "message": f"æ­¥éª¤ 4: æ‰€æœ‰é…æ–¹å·²ç”Ÿæˆï¼æ­£åœ¨å¯åŠ¨ Scrapy çˆ¬è™«æŠ“å–æ•°æ®..."}
            print("\n" + "="*50)
            print(JOB_STATUS["message"])
            
            s = {
                # 1. æ ¸å¿ƒè®¾ç½®
                'BOT_NAME': 'university_scraper',
                'SPIDER_MODULES': ['university_scraper.university_scraper.spiders'],
                'NEWSPIDER_MODULE': 'university_scraper.university_scraper.spiders',
                
                # 2. çˆ¬è™«è®¾ç½®
                'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
                'ROBOTSTXT_OBEY': False,
                'DEFAULT_REQUEST_HEADERS': {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                },
                
                # 3. Pipeline å’Œæ•°æ®åº“
                'ITEM_PIPELINES': {
                   # !! å…³é”®ä¿®å¤ï¼šå¿…é¡»ä½¿ç”¨åŒå±‚åµŒå¥—è·¯å¾„ !!
                   'university_scraper.university_scraper.pipelines.MongoPipeline': 300,
                },
                'MONGO_URI': 'mongodb://localhost:27017/',
                'MONGO_DB': 'teacher_db',
                'MONGO_COLLECTION': 'pku_cs_teachers',
                
                # 4. API å¯†é’¥ (ä»æœ¬æ–‡ä»¶é¡¶éƒ¨çš„å…¨å±€å˜é‡è¯»å–)
                'DEEPSEEK_API_KEY': DEEPSEEK_API_KEY, 
                
                # 5. Asyncio å’Œæ—¥å¿—
                'TWISTED_REACTOR': "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
                'LOG_LEVEL': 'INFO', 
            }
            
            process = CrawlerProcess(s)
            process.crawl(PkuGenericSpider)
            process.start() # <-- è¿™ä¼šé˜»å¡ï¼Œç›´åˆ°çˆ¬è™«è¿è¡Œå®Œæ¯•
            
            JOB_STATUS = {"status": "finished", "message": f"å…¨æµç¨‹è‡ªåŠ¨åŒ–ä»»åŠ¡å·²å®Œæˆï¼å·²çˆ¬å–æ‰€æœ‰å­¦é™¢ã€‚"}
            print("\n" + "="*50)
            print("ğŸ¤– [å…¨æµç¨‹è‡ªåŠ¨åŒ–ä»»åŠ¡å·²å®Œæˆ]")
            print("="*50 + "\n")

        except Exception as e:
            print(f"!! [å…¨æµç¨‹è‡ªåŠ¨åŒ–ä»»åŠ¡å¤±è´¥] !!: {e}")
            JOB_STATUS = {"status": "error", "message": f"ä»»åŠ¡å¤±è´¥: {e}"}

# ( ... _generate_recipe_logic å’Œ generate_recipe_manual ... )
# ( ... è¿™ä¸¤ä¸ªå‡½æ•°å’Œä¹‹å‰ä¸€æ ·ï¼Œæ— éœ€æ”¹åŠ¨ ... )
def _generate_recipe_logic(url, college_name):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ...'}
        with httpx.Client(headers=headers, follow_redirects=True, timeout=10.0) as http_client:
            response = http_client.get(url)
            response.raise_for_status() 
            html_content = response.text
    except Exception as e:
        print(f"    [AIé…æ–¹] ä¸‹è½½ {url} å¤±è´¥: {e}")
        raise
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        for tag in soup(['nav', 'header', 'footer', 'aside', 'script', 'style', 'head', 'link']):
            tag.decompose()
        if soup.body:
            clean_html_tags = soup.body.prettify()
        else:
            clean_html_tags = soup.prettify()
        html_snippet = f"å¸¦æ ‡ç­¾çš„ HTML (ç”¨äºæ‰¾é€‰æ‹©å™¨): {clean_html_tags[:30000]}"
    except Exception as e:
        html_snippet = html_content[html_content.find('<body>'):html_content.find('</body>')][:30000]

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Webçˆ¬è™«å·¥ç¨‹å¸ˆã€‚ä¸‹é¢æ˜¯ä¸€ä»½è¢«æ¸…æ´—è¿‡çš„å¤§å­¦å¸ˆèµ„é¡µé¢çš„ HTML æºä»£ç ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ­¤HTMLï¼Œå¹¶è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«ç”¨äºçˆ¬å–å®ƒçš„å…³é”®CSSé€‰æ‹©å™¨ã€‚
    1. æ‰¾åˆ°åŒ…å«äº†ã€æ¯ä¸€ä½ã€‘æ•™å¸ˆä¿¡æ¯çš„ã€å¯é‡å¤çš„HTMLå…ƒç´ ã€‚è¿™å°†ä½œä¸º "list_selector"ã€‚
    2. åœ¨è¿™ä¸ª "list_selector" å…ƒç´ ã€å†…éƒ¨ã€‘ï¼Œæ‰¾åˆ°åŒ…å«äº†æ‰€æœ‰ç›¸å…³æ–‡æœ¬ï¼ˆå§“åã€èŒç§°ã€ç®€ä»‹ç­‰ï¼‰çš„é‚£ä¸ªå­å…ƒç´ ã€‚è¿™å°†ä½œä¸º "text_selector"ã€‚
    HTMLæºä»£ç ï¼ˆå·²æ¸…æ´—ï¼‰:
    ```html
    {html_snippet}
    ```
    è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦åŒ…å«ä»»ä½•å¤šä½™çš„è§£é‡Šï¼š
    {{
      "list_selector": "...",
      "text_selector": "..."
    }}
    """
    try:
        print(f"    [AIé…æ–¹] æ­£åœ¨è¯·æ±‚ DeepSeek AI åˆ†æ(å·²æ¸…æ´—çš„) HTML...")
        completion = ai_client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        response_content = completion.choices[0].message.content
        ai_recipe = json.loads(response_content)
        list_selector = ai_recipe.get('list_selector')
        text_selector = ai_recipe.get('text_selector')
        if not list_selector or not text_selector:
            raise Exception("AI æœªèƒ½è¿”å›æœ‰æ•ˆçš„é€‰æ‹©å™¨")
        print(f"    [AIé…æ–¹] AI ç”Ÿæˆé…æ–¹æˆåŠŸ: {ai_recipe}")
    except Exception as e:
        print(f"    [AIé…æ–¹] AI é…æ–¹ç”Ÿæˆå¤±è´¥: {e}")
        raise
    return {
        "college": college_name,
        "page_name": f"å…¨è‡ªåŠ¨ç”Ÿæˆ (v4)",
        "start_url": url,
        "list_selector": list_selector,
        "text_selector": text_selector
    }

@app.route('/api/generate-recipe', methods=['POST'])
def generate_recipe_manual():
    if not ai_client:
        return jsonify({'status': 'error', 'message': 'AI å®¢æˆ·ç«¯æœªåˆå§‹åŒ–'}), 500
    data = request.json
    url = data.get('url')
    college_name = data.get('college')
    if not url or not college_name:
        return jsonify({'status': 'error', 'message': 'ç¼ºå°‘ URL æˆ–å­¦é™¢åç§°'}), 400
    try:
        recipe = _generate_recipe_logic(url, college_name)
        recipe_collection.update_one(
            {'start_url': recipe['start_url']},
            {'$set': recipe},
            upsert=True
        )
        return jsonify({'status': 'success', 'message': 'AI é…æ–¹å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜ï¼(v4)', 'recipe': recipe})
    except Exception as e:
        print(f"æ‰‹åŠ¨é…æ–¹ç”Ÿæˆå¤±è´¥: {e}")
        return jsonify({'status': 'error', 'message': f'æ‰‹åŠ¨é…Gæ–¹ç”Ÿæˆå¤±è´¥: {e}'}), 500


# --- å¯åŠ¨æœåŠ¡å™¨ ---
if __name__ == '__main__':
    # ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£(0.0.0.0)ï¼Œå…è®¸ä»å…¶ä»–è®¾å¤‡è®¿é—®ï¼ˆåŒ…æ‹¬å¾®ä¿¡å°ç¨‹åºï¼‰
    # å¦‚æœåªæƒ³æœ¬åœ°è®¿é—®ï¼Œå¯ä»¥ä½¿ç”¨ host='127.0.0.1'
    print("\n" + "="*50)
    print("ğŸš€ Flask æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    print("="*50)
    print(f"ğŸ“¡ æœ¬åœ°è®¿é—®: http://127.0.0.1:5000")
    print(f"ğŸ“¡ ç½‘ç»œè®¿é—®: http://ä½ çš„IPåœ°å€:5000")
    print("="*50 + "\n")
    app.run(debug=True, use_reloader=False, host='0.0.0.0', port=5000)